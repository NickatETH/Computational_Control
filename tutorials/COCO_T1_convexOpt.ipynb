{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUAeWWcBNYOE"
   },
   "source": [
    "# Computational control - Tutorial 1\n",
    "\n",
    "We illustrate how to solve convex optimization programs using the open source Python-embedded modeling language CVXPY ([link](https://www.cvxpy.org/index.html#)).\n",
    "\n",
    "We first look at a least squares problem and subsequently consider a constrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orange\"> WARNING: you must ensure that the kernel (upper-right of the JupyterHub window) is set to the ``Computational Control'' kernel before running this code. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYRmSrAhR1WM"
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import seaborn as sns  # Import seaborn\n",
    "sns.set_style('white') # Set the seaborn style to 'white' to mimic seaborn-white\n",
    "# import seaborn\n",
    "# plt.style.use('seaborn-white')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(1)\n",
    "# Set nice printing format\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z56RonEnTf2D"
   },
   "source": [
    "# Least squares problems\n",
    "\n",
    "A  _least squares problem_ is an optimization problem with no constraints\n",
    "and a cost function which is a sum of squares of affine terms in the unknown decision variable:\n",
    "\n",
    "$$  \n",
    "\\begin{equation} \\tag{LS}\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize}   & \\lVert Ax - b \\rVert_2^2\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$  are problem data and $x \\in \\mathbb{R}^{n}$ is the decision variable.  \n",
    "\n",
    "Least squares problems are ubiquituous in the context of data fitting.  For example, consider the model\n",
    "\n",
    "$$\n",
    "y =Ax+ \\nu\n",
    "$$\n",
    "\n",
    "where $y \\in \\mathbb{R}^m$ is a vector of measurements, $x \\in \\mathbb{R}^{n}$ is a vector of parameters to be estimated, and $\\nu \\in \\mathbb{R}^m$ is measurement error that is unknown, but assumed to be small (in the norm $\\lVert \\, \\cdot \\, \\rVert_2$). In this context, it makes sense to pose the estimation problem as that of finding $x^* \\in \\mathbb{R}^{n}$ such that\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x^* \\in\n",
    "    \\begin{array}{ll}\n",
    "    \\arg\\mbox{min}   & \\lVert Ax - y \\rVert_2^2 .\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "The number of measurements $m$ is usually much larger than the number of parameters $n,$ i.e. $m\\gg n$. Least squares problems can be therefore seen as a tool to approximate the solution of overdetermined systems, i.e. sets of equations in which there are more equations than unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4nGJpU0NNdt",
    "outputId": "8c77dfcd-6477-4e00-8636-43097123ae72"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "m = 15\n",
    "n = 10\n",
    "\n",
    "# Generate data\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)\n",
    "\n",
    "# Variables\n",
    "x = cp.Variable(n)\n",
    "# Cost\n",
    "cost = cp.sum_squares(A @ x - b)\n",
    "# Constraints\n",
    "constraints = [];\n",
    "# Problem\n",
    "prob = cp.Problem(cp.Minimize(cost),constraints)\n",
    "# Solution\n",
    "prob.solve()\n",
    "\n",
    "# Print result\n",
    "print(\"\\nThe optimal value is\", prob.value)\n",
    "print(\"A solution is x =\",x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L14qLsiGcM1"
   },
   "source": [
    "### Constraints\n",
    "Constraints arise for a variety of reasons and can be naturally added to the basic least squares problem (LS). For example, constraints arise\n",
    "\n",
    "*   to rule out certain unacceptable approximations;\n",
    "*   to incorporate prior information about desired estimates;\n",
    "*   to encode information about the geometry of a problem.\n",
    "\n",
    "For example, consider the optimization problem\n",
    "\n",
    "$$  \n",
    "\\begin{equation} \\tag{LS-C}\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize}   & \\lVert Ax - b \\rVert^2 \\\\\n",
    "    \\mbox{subject to} & \\lVert x \\rVert_\\infty  \\leq \\alpha.\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$  are problem data, $x \\in \\mathbb{R}^{n}$ is the decision variable, and $\\alpha >0$ is a user-defined parameter.\n",
    "\n",
    "The constraint $\\lVert x \\rVert_\\infty  \\leq \\alpha$ ensures that the solution $x$ is such that $-\\alpha \\le x_i \\le \\alpha.$\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "\n",
    "Regularization is another technique to incorporate prior information by adding extra terms to the cost function of the original problem.\n",
    "\n",
    "For example, the least squares problem (LS) can be regularized to find a solution $x$ that makes the residual $\\lVert Ax âˆ’ b \\rVert$ small and is itself small, if possible. In particular, _Tikhonov regularization_ turns (LS) into the optimization problem\n",
    "\n",
    "$$  \n",
    "\\begin{equation} \\tag{LS-TR}\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize}   & \\lVert Ax - b \\rVert_2^2 +\\lambda \\lVert x \\rVert_2^2\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\lambda >0 $ is a user-defined parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dUucuqR8Qvsh"
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "m = 15\n",
    "n = 2\n",
    "\n",
    "# Generate random data to set up the exercise\n",
    "A = np.random.randn(m, n)\n",
    "b = np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "UcoKUSquUoys",
    "outputId": "12fbf768-91d9-469a-99fe-4ccc7aeb2328"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Problem parameters\n",
    "Lambda = 10\n",
    "alpha = 1\n",
    "\n",
    "############################ Your code goes here ###############################\n",
    "\n",
    "# Variables\n",
    "x = cp.Variable(n)\n",
    "# Cost\n",
    "cost = cp.sum_squares(A @ x - b) #+ Lambda*cp.norm(x,2)**2   # Add for (LS-TR)\n",
    "# Constraints\n",
    "constraints = []\n",
    "# constraints += [cp.norm(x,'inf') <= alpha] # Add for (LS-C)\n",
    "# Problem\n",
    "prob = cp.Problem(cp.Minimize(cost),constraints)\n",
    "# Solution\n",
    "prob.solve()\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Print result\n",
    "print(\"A solution is x =\",x.value,\"\\n\")\n",
    "if constraints and np.linalg.norm(x.value,np.inf)<=alpha:\n",
    "  print(\"Constraints are satisfied\")\n",
    "print(\"\\nThe optimal value is\", prob.value,\"\\n\")\n",
    "\n",
    "# Visualize solution and constraint set\n",
    "print(\"Here's a plot showing the location of the solution:\\n\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-alpha, alpha)\n",
    "ax.set_ylim(-alpha, alpha)\n",
    "\n",
    "if constraints:\n",
    "    # Plot constraint set\n",
    "    ax.add_patch(Polygon([[-alpha,-alpha],\n",
    "                          [-alpha, alpha],\n",
    "                          [ alpha, alpha],\n",
    "                          [ alpha, -alpha]],\n",
    "                          closed=True,\n",
    "                          color= \"cornflowerblue\",\n",
    "                          fill=True))\n",
    "\n",
    "# Plot solution\n",
    "ax.plot(x.value[0],\n",
    "          x.value[1],\n",
    "          marker=\"o\",\n",
    "          markersize=7.5,\n",
    "          markeredgecolor=\"black\",\n",
    "          markerfacecolor=\"coral\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRsoX-B6SRWh"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Define and solve the optimization problems (LS-C) and (LS-TR) for given random data and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT4CAEe2BG9r"
   },
   "source": [
    "# Constrained optimization\n",
    "\n",
    "Consider the constrained optimization problem\n",
    "\n",
    "$$  \n",
    "\\begin{equation} \\tag{CO}\n",
    "    \\begin{array}{ll}\n",
    "    \\mbox{minimize}   & f(x) \\\\\n",
    "    \\mbox{subject to} & g(x) \\le 0\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "with decision variable $x \\in \\mathbb{R}^2,$ cost function\n",
    "\n",
    "$$\n",
    "f(x) = \\left( x_1 -3/2\\right)^2 + \\left( x_2 -2\\right)^2\n",
    "$$\n",
    "\n",
    "and constraint set defined by\n",
    "\n",
    "$$\n",
    "g(x) =\n",
    "\\begin{pmatrix}\n",
    "-x_1 \\\\\n",
    "-x_2 \\\\\n",
    "-1+x_1+x_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The cost function of problem (CO) is convex. The inequality constraints of problem (CO) define a convex set (whose boundary is a triangle). Problem (CO) is therefore convex and easily solved using CVXPY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwqaNPBwEZmw",
    "outputId": "540546bd-f71d-4d66-cc48-2b7fce194c7e"
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "x = cp.Variable(2)\n",
    "# Cost\n",
    "cost = (x[0]-1.5)**2 +(x[1]-2)**2\n",
    "# Constraints\n",
    "constraints = [-x[0] <=0, -x[1] <=0, -1+x[0]+x[1] <=0];\n",
    "# Problem\n",
    "prob = cp.Problem(cp.Minimize(cost),constraints)\n",
    "# Solution\n",
    "prob.solve()\n",
    "\n",
    "# Print result\n",
    "print(\"A solution is x =\",x.value,\"\\n\")\n",
    "if np.linalg.norm(x.value,np.inf)<=alpha:\n",
    "  print(\"Constraints are satisfied\")\n",
    "print(\"\\nThe optimal value is\", prob.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fpwV4NIEcge"
   },
   "source": [
    "We could have guessed the the solution by visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "eU5seI26243o",
    "outputId": "04822e32-ba81-4376-bfdc-6bde98a9943c"
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def f(x1, x2):\n",
    "    return (x1-1.5)**2+(x2-2)**2\n",
    "\n",
    "# Define meshgrid\n",
    "x1 = np.linspace(0, 1.5, 50)\n",
    "x2 = np.linspace(0, 1.5, 40)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "# Contour plot of function\n",
    "fig, ax = plt.subplots()\n",
    "levels = [0.8, 1.6, 2.4, 3.125, 3.75, 4.75];\n",
    "CS = ax.contour(X1, X2, f(X1, X2), levels=levels,colors='black')\n",
    "ax.clabel(CS, inline=1, fontsize=10)\n",
    "# Add constraint set\n",
    "ax.add_patch(Polygon([[0,0],[0,1],[1,0]],\n",
    "                     closed=True,\n",
    "                     color= \"cornflowerblue\",\n",
    "                     fill=True))\n",
    "# Plot solution\n",
    "ax.plot(x.value[0],\n",
    "        x.value[1],\n",
    "        marker=\"o\",\n",
    "        markersize=7.5,\n",
    "        markeredgecolor=\"black\",\n",
    "        markerfacecolor=\"coral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aam2heU836ve",
    "outputId": "ab2ddce9-18e3-439e-c09d-55844a741f7e"
   },
   "outputs": [],
   "source": [
    "############################ Your code goes here ###############################\n",
    "\n",
    "print(\"optimal (g_3(x) <= 0) dual variable\", constraints[2].dual_value)\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTlbUzLxEjec"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Let's show that the optimal point $x^{*}$ satisfies the [KKT](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) conditions.\n",
    "\n",
    "How to do that? Well, this amounts to showing that the gradient of the cost function $\\nabla f(x)$ is parallel to the normal vector to the side of the triangle on which $x^{*}$ lies...\n",
    "\n",
    "In your investigations, consider the following steps:\n",
    "\n",
    "1.   Compute the value of the multiplier $\\mu^{*}$ that makes the two gradients equal and opposite, i.e.\n",
    "\n",
    "$$\n",
    "\\nabla f(x^{*}) +\\mu^{*} \\nabla g_3(x^{*}) = 0\n",
    "$$\n",
    "\n",
    "2. Are the constraints defined by the inequalities $g_1(x)\\le 0$ and $g_2(x)\\le 0$ active?\n",
    "3. Check whether the multiplier $\\mu^{*}$ is actually the correct one using CVX. [Hint: Check the value of each constraints[i].dual_value]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ZphNUARxTMr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Computational Control",
   "language": "python",
   "name": "coco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
